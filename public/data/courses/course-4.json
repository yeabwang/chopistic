{
  "id": 2,
  "title": "Neural Networks from Scratch",
  "category": "Deep Learning",
  "level": "Beginner to Intermediate",
  "duration": "Unknown",
  "description": "Learn how to build a neural network from scratch in Python. A foundational course that covers the fundamental concepts of deep learning without relying on major frameworks.",
  "instructor": "Harrison Kinsley (sentdex)",
  "enrollmentCount": 0,
  "rating": 0,
  "media": "videos/feature-nn.mp4",
  "thumbnail": "img/nn-gallery-1.webp",
  "chapters": [
    {
      "id": 1,
      "title": "Intro and Neuron Code",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/Wo5dMEP_BbI?si=p39ZCHQlcC2ZS9RU",
      "content": {
        "overview": "Introduces the motivation behind building a neural network from scratch rather than using pre-built libraries, providing intuition for the mechanics of neural nets.",
        "sections": [
          {
            "title": "Objective: Why Build a Neural Network From Scratch?",
            "content": "Explains the importance of understanding fundamentals to customize, troubleshoot, and innovate."
          },
          {
            "title": "Core Concepts of Neural Networks",
            "content": "Neurons, weights, biases, layers, input/output, and training goal."
          },
          {
            "title": "From Mathematical Formulas to Code",
            "content": "Translating summation and matrix notations into simple Python code."
          },
          {
            "title": "Practical Implementation: Coding the First Neuron",
            "content": "First Python implementation of a neuron."
          }
        ],
        "keyPoints": [
          "Neurons = weighted inputs + bias → activation.",
          "Training adjusts weights/biases to minimize error.",
          "Math formulas → simple code."
        ],
        "practicalExercise": "Write Python code to implement a single neuron with initialized weights and bias."
      }
    },
    {
      "id": 2,
      "title": "Forward Pass Through a Network",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/lGLto9Xd7bU",
      "content": {
        "overview": "Builds from a single neuron to multiple neurons organized in layers. Introduces forward pass logic across layers.",
        "sections": [
          {
            "title": "From Neuron to Layer",
            "content": "Grouping neurons and connecting them to form hidden layers."
          },
          {
            "title": "Forward Pass Logic",
            "content": "How inputs propagate through weights/biases to produce outputs."
          },
          {
            "title": "Coding a Forward Pass",
            "content": "Python implementation of a forward pass through multiple layers."
          }
        ],
        "keyPoints": [
          "Multiple neurons form a layer.",
          "Forward pass is repeating weighted sum → activation across layers.",
          "Output depends on structure and parameters."
        ],
        "practicalExercise": "Implement a forward pass through a 2-layer network in Python."
      }
    },
    {
      "id": 3,
      "title": "Activation Functions",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/tMrbN67U9d4",
      "content": {
        "overview": "Explains why non-linearity is needed and introduces common activation functions.",
        "sections": [
          {
            "title": "Why Non-Linearity?",
            "content": "Without non-linear activation, networks are just linear combinations."
          },
          {
            "title": "Popular Functions",
            "content": "Sigmoid, ReLU, Tanh, Softmax."
          },
          {
            "title": "Implementation in Python",
            "content": "Coding these functions and applying them to outputs."
          }
        ],
        "keyPoints": [
          "Activations give networks the ability to model complex functions.",
          "Sigmoid for probabilities, ReLU for efficiency, Softmax for classification."
        ],
        "practicalExercise": "Implement Sigmoid, ReLU, and Softmax in Python and test them on sample inputs."
      }
    },
    {
      "id": 4,
      "title": "Loss Functions",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/TEWy9vZcxW4",
      "content": {
        "overview": "Introduces error measurement in neural networks and common loss functions.",
        "sections": [
          {
            "title": "Why Loss Functions?",
            "content": "Loss measures how far predictions are from labels."
          },
          {
            "title": "Common Functions",
            "content": "Mean Squared Error (MSE), Categorical Cross-Entropy."
          },
          {
            "title": "Python Implementation",
            "content": "Implementing loss functions and testing with predictions."
          }
        ],
        "keyPoints": [
          "Loss guides training by providing error feedback.",
          "Different tasks → different losses."
        ],
        "practicalExercise": "Implement MSE and Cross-Entropy in Python."
      }
    },
    {
      "id": 5,
      "title": "Backpropagation Basics",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/gmjzbpSVY1A",
      "content": {
        "overview": "Introduces the idea of gradients and error propagation backward through the network.",
        "sections": [
          {
            "title": "The Need for Backpropagation",
            "content": "To adjust weights, we must know how errors depend on them."
          },
          {
            "title": "Gradient Concept",
            "content": "Derivatives measure how a small change affects loss."
          },
          {
            "title": "Step-by-Step Process",
            "content": "Errors flow backward, gradients are computed for each weight."
          }
        ],
        "keyPoints": [
          "Backpropagation is chain rule in action.",
          "Enables training by updating weights systematically."
        ],
        "practicalExercise": "Walk through derivative calculation of a simple 2-layer network."
      }
    },
    {
      "id": 6,
      "title": "Optimization Techniques",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/omz_NdFgWyU",
      "content": {
        "overview": "Shows how to use gradients to adjust weights with optimizers.",
        "sections": [
          {
            "title": "Gradient Descent",
            "content": "The simplest optimization algorithm."
          },
          {
            "title": "Variants",
            "content": "Stochastic, Mini-batch, Adam optimizer."
          },
          {
            "title": "Implementation",
            "content": "Update rules coded in Python."
          }
        ],
        "keyPoints": [
          "Learning rate critical for convergence.",
          "Adam is often preferred for efficiency."
        ],
        "practicalExercise": "Implement basic gradient descent and experiment with learning rates."
      }
    },
    {
      "id": 7,
      "title": "Building a Full Network",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/dEXPMQXoiLc",
      "content": {
        "overview": "Brings all components together into a working neural network.",
        "sections": [
          {
            "title": "From Layers to Network",
            "content": "Combining neurons, activations, loss, and optimization."
          },
          {
            "title": "Step-by-Step Training Loop",
            "content": "Forward pass → loss → backprop → update."
          },
          {
            "title": "Python Implementation",
            "content": "Build a simple network for classification."
          }
        ],
        "keyPoints": [
          "All previous concepts unify here.",
          "Training loop is the core cycle of neural nets."
        ],
        "practicalExercise": "Code a small network to classify 2D points."
      }
    },
    {
      "id": 8,
      "title": "Improving Performance",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/levekYbxauw",
      "content": {
        "overview": "Techniques for better results and avoiding pitfalls.",
        "sections": [
          {
            "title": "Regularization",
            "content": "Dropout, L2 penalty."
          },
          {
            "title": "Data Augmentation",
            "content": "Expanding dataset to prevent overfitting."
          },
          {
            "title": "Batch Normalization",
            "content": "Stabilizes training."
          }
        ],
        "keyPoints": [
          "Overfitting is a major concern.",
          "Regularization and augmentation help generalization."
        ],
        "practicalExercise": "Add dropout to your previous network implementation."
      }
    },
    {
      "id": 9,
      "title": "Final Project and Wrap-Up",
      "duration": "60 minutes",
      "videoUrl": "https://www.youtube.com/embed/txh3TQDwP1g",
      "content": {
        "overview": "Culminates in building a complete neural network from scratch for a real classification problem.",
        "sections": [
          {
            "title": "Project Overview",
            "content": "Using MNIST-like dataset for digit classification."
          },
          {
            "title": "Implementation",
            "content": "End-to-end pipeline with forward pass, loss, backprop, optimization."
          },
          {
            "title": "Reflection",
            "content": "Recap of journey and importance of fundamentals."
          }
        ],
        "keyPoints": [
          "From neuron to full project.",
          "Concepts apply beyond toy datasets."
        ],
        "practicalExercise": "Build and train a full network for digit classification."
      }
    }
  ],
  "prerequisites": [
    "Intermediate Python programming knowledge",
    "Basic understanding of mathematics (vectors, matrices, derivatives)"
  ],
  "learningOutcomes": [
    "Understand the fundamental components of a neural network (neurons, layers, activation functions).",
    "Implement a forward pass through a neural network.",
    "Understand and implement loss functions.",
    "Grasp the concept of backpropagation and optimization.",
    "Build a complete neural network to solve a classification problem."
  ],
  "resources": [],
  "quiz": {
    "available": true,
    "questionsCount": 10,
    "passingScore": 70,
    "timeLimit": 10
  }
}